{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78025c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "import torch\n",
    "import torchvision.transforms as tf\n",
    "import pdb\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d1ab565",
   "metadata": {},
   "outputs": [],
   "source": [
    "from MyNeuralNetwork import NeuralNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af8decd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetDatasets(download=True):\n",
    "    dataset_train = datasets.FashionMNIST(\n",
    "        root = r'data/train',\n",
    "        download = download,\n",
    "        train = True,\n",
    "        transform = tf.ToTensor() \n",
    "    )\n",
    "    \n",
    "    dataset_test = datasets.FashionMNIST(\n",
    "        root = r'data/test',\n",
    "        download = download,\n",
    "        train = False,\n",
    "        transform = tf.ToTensor() \n",
    "    )\n",
    "    return dataset_train, dataset_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b604b987",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train, dataset_test = GetDatasets(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "224e58a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 500\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2ace5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "network = NeuralNetwork().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "392c1575",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(network.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4e7f766",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1db8f4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.5, Epoch loss: 2.064820403812312\n",
      "Learning rate: 0.5, Epoch loss: 1.7848527701962895\n",
      "Learning rate: 0.5, Epoch loss: 1.7167900610370797\n",
      "Learning rate: 0.5, Epoch loss: 1.6957913496915031\n",
      "Learning rate: 0.5, Epoch loss: 1.6859249978506265\n",
      "Learning rate: 0.5, Epoch loss: 1.6811046199638302\n",
      "Learning rate: 0.5, Epoch loss: 1.6740297369596338\n",
      "Learning rate: 0.5, Epoch loss: 1.67105129085669\n",
      "Learning rate: 0.5, Epoch loss: 1.6673994074348641\n",
      "Learning rate: 0.5, Epoch loss: 1.6652306959408671\n",
      "Learning rate: 0.05, Epoch loss: 1.6579954083226307\n",
      "Learning rate: 0.05, Epoch loss: 1.6573620163092093\n",
      "Learning rate: 0.05, Epoch loss: 1.6569696204001163\n",
      "Learning rate: 0.05, Epoch loss: 1.6567368928123922\n",
      "Learning rate: 0.05, Epoch loss: 1.6564426712629174\n",
      "Learning rate: 0.05, Epoch loss: 1.6561844338889884\n",
      "Learning rate: 0.05, Epoch loss: 1.6558740990502494\n",
      "Learning rate: 0.05, Epoch loss: 1.6556233778721143\n",
      "Learning rate: 0.05, Epoch loss: 1.6553639494070487\n",
      "Learning rate: 0.05, Epoch loss: 1.6552147735066776\n",
      "Learning rate: 0.005, Epoch loss: 1.6547703101855367\n",
      "Learning rate: 0.005, Epoch loss: 1.6546465579201193\n",
      "Learning rate: 0.005, Epoch loss: 1.6546327045985632\n",
      "Learning rate: 0.005, Epoch loss: 1.6546096411071907\n",
      "Learning rate: 0.005, Epoch loss: 1.6545759750013591\n",
      "Learning rate: 0.005, Epoch loss: 1.6545593648397623\n",
      "Learning rate: 0.005, Epoch loss: 1.654528757103351\n",
      "Learning rate: 0.005, Epoch loss: 1.6545142606526864\n",
      "Learning rate: 0.005, Epoch loss: 1.6544887779139672\n",
      "Learning rate: 0.005, Epoch loss: 1.6544649300455045\n",
      "Learning rate: 0.0005, Epoch loss: 1.6544082785854821\n",
      "Learning rate: 0.0005, Epoch loss: 1.6544051180366708\n",
      "Learning rate: 0.0005, Epoch loss: 1.6544022099310611\n",
      "Learning rate: 0.0005, Epoch loss: 1.6543991265176725\n",
      "Learning rate: 0.0005, Epoch loss: 1.6543963526477332\n",
      "Learning rate: 0.0005, Epoch loss: 1.654394022556914\n",
      "Learning rate: 0.0005, Epoch loss: 1.6543910323070878\n",
      "Learning rate: 0.0005, Epoch loss: 1.654389318297891\n",
      "Learning rate: 0.0005, Epoch loss: 1.6543858572214591\n",
      "Learning rate: 0.0005, Epoch loss: 1.654383852702229\n"
     ]
    }
   ],
   "source": [
    "for lr in [0.5, 0.05, 0.005, 0.0005]:\n",
    "    optimizer = torch.optim.SGD(network.parameters(), lr=lr)\n",
    "    \n",
    "    log_dir = './logs/{}'.format(lr)\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        epoch_loss = 0\n",
    "        epoch_accuracy = 0\n",
    "        \n",
    "        for idx, batch in enumerate(dataloader_train):\n",
    "            # zero the gradient\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            x, y = batch[0], batch[1]\n",
    "\n",
    "            # flatten it first\n",
    "            x = torch.flatten(x, start_dim = 1)\n",
    "\n",
    "            # forward pass\n",
    "            predictions = network(x)\n",
    "\n",
    "            loss = criterion(predictions, y)\n",
    "\n",
    "            # backpropogate the loss\n",
    "            loss.backward()\n",
    "\n",
    "            # step the optimizer in the direction of the gradient\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        writer.add_scalar('Epoch loss', epoch_loss / idx, epoch)\n",
    "        writer.flush()\n",
    "        \n",
    "        # debug following code\n",
    "        for idx, batch in enumerate(dataloader_test):\n",
    "            \n",
    "            network.eval()\n",
    "            \n",
    "            x, y = batch[0].to(device), batch[1].to(device)\n",
    "            x = torch.flatten(x, start_dim = 1)\n",
    "            predcitions = network(x)\n",
    "            \n",
    "            pred_label = torch.argmax(predictions, dim=1)\n",
    "            n_correct = (y==pred_label).float()\n",
    "            accuracy = n_correct.sum() / n_correct.numel()\n",
    "            epoch_accuracy += 100 * accuracy\n",
    "            \n",
    "        writer.add_scalar('Epoch accuracy', epoch_accuracy / idx, epoch)\n",
    "        writer.flush()\n",
    "        \n",
    "        print('Learning rate: {}, Epoch loss: {:f}, accuracy: {:f}'.format(lr, epoch_loss / idx, epoch_accuracy))\n",
    "        writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775c8c09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
